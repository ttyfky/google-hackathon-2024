{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of MOZ\n",
    "\n",
    "## HOW TO USE\n",
    "\n",
    "1. set 2 variables via environment variable or standart input.\n",
    "    1. `PRED_SS_ID` evaluation target google sheet id\n",
    "    2. `GOOGLE_CLOUD_PROJECT` google cloud project id\n",
    "    3. `TRUTH_DATA_ID` bigquery table id which holds ground truth\n",
    "2. execute all the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "PRED_SS_ID = os.environ.get('PRED_SS_ID') or input(\"Input Prediction Google Sheet ID\")\n",
    "PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT') or input('Input Google Cloud Project ID')\n",
    "TRUTH_DATA_ID = os.environ.get('TRUTH_BQ_TABLE_ID') or input(\"Input Truth Data BigQuery Table ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 予測データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from b_moz.libs.io.google import GoogleSpreadSheet, GoogleDriveAuth\n",
    "\n",
    "from gspread_dataframe import get_as_dataframe # type: ignore\n",
    "\n",
    "def load_pred_data(ss_id):\n",
    "    GoogleDriveAuth.get_credentials(\n",
    "        os.environ.get(\"CLIENT_ID\"), os.environ.get(\"CLIENT_SECRET\")  # type: ignore\n",
    "    )\n",
    "    client = GoogleSpreadSheet.get_client()\n",
    "\n",
    "    # prediction data\n",
    "    pred_gsheet = client.open_by_key(ss_id)\n",
    "    return pred_gsheet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正解データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_gbq as pbq\n",
    "import polars as pl\n",
    "\n",
    "def load_truth_data():\n",
    "    sql = (\n",
    "        f\"\"\"\n",
    "        select * from `{TRUTH_DATA_ID}`\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    ground_truth_df = pl.from_pandas(\n",
    "        pbq.read_gbq(sql, project_id=PROJECT_ID, dialect=\"standard\", progress_bar_type=None) # type: ignore\n",
    "    )\n",
    "    return ground_truth_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation\n",
    "\n",
    "マルチラベル分類の評価手法に則り、下記指標を用いて評価\n",
    "\n",
    "- $T_i$ $i$ 番目のモデルの正解の色名の集合\n",
    "- $Y_i$ $i$ 番目のモデルの予測した色名の集合\n",
    "\n",
    "\n",
    "1. EM (Exact Match)\n",
    "    - $\n",
    "        \\frac{1}{N} \\sum_{i=1}^{N} I[Y_i = T_i]\n",
    "      $\n",
    "    - モデルごとに全ての色名を正確に予測した割合\n",
    "2. accuracy 正解率\n",
    "    - $\n",
    "        \\frac{1}{N} \\sum_{i=1}^{N} \\frac{|Y_i \\cap T_i|}{|Y_i \\cup T_i|}\n",
    "      $\n",
    "    - 予測した色名のうち、正しく予測できていたものの割合\n",
    "3. recall 再現率\n",
    "    - $\n",
    "        \\frac{1}{N} \\sum_{i=1}^{N} \\frac{|Y_i \\cap T_i|}{|T_i|}\n",
    "      $\n",
    "    - 正解に含まれる色名のうち、実際に予測できたものの割合\n",
    "4. precision 適合率\n",
    "    - $\n",
    "        \\frac{1}{N} \\sum_{i=1}^{N} \\frac{|Y_i \\cap T_i|}{|Y_i|}\n",
    "      $\n",
    "    - 予測した色名のうち、正解に含まれていたものの割合\n",
    "    - 正解の色名に予測した色名が全て含まれている場合にも、余分な色名を予測しているとスコアが小さくなる\n",
    "5. F1 score F値\n",
    "     - $\n",
    "        \\frac{ 2 \\times precision \\times recall}{precision + recall}\n",
    "       $\n",
    "    - precision と recall の調和平均\n",
    "\n",
    "### c.f.\n",
    "\n",
    "- <https://zero2one.jp/ai-word/accuracy-precision-recall-f-measure/>\n",
    "- <https://qiita.com/jyori112/items/110596b4f04e4e1a3c9b#%E5%A4%9A%E3%83%A9%E3%83%99%E3%83%AB%E5%88%86%E9%A1%9E%E3%82%BF%E3%82%B9%E3%82%AFmulti-label-classification>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(truth: set, pred: set) -> bool:\n",
    "    return truth == pred\n",
    "\n",
    "def accuracy(truth: set, pred: set) -> float:\n",
    "    if len(truth | pred) == 0:\n",
    "        return 0\n",
    "    return len(truth & pred) / len(truth | pred)\n",
    "\n",
    "def recall(truth: set, pred: set) -> float:\n",
    "    if len(truth) == 0:\n",
    "        return 0\n",
    "    return len(truth & pred) / len(truth)\n",
    "\n",
    "def precision(truth: set, pred: set) -> float:\n",
    "    if len(pred) == 0:\n",
    "        return 0\n",
    "    return len(truth & pred) / len(pred)\n",
    "\n",
    "def calc_metrics(validation_df: pl.DataFrame):\n",
    "    # calclate metrics per model\n",
    "    eval_indicators = []\n",
    "    for category, model_name, true_labels, pred_labels in validation_df.select(\"category\", \"model\", \"truth\", \"pred\").iter_rows():\n",
    "        truth: set = set(true_labels or [])\n",
    "        pred: set = set(pred_labels or [])\n",
    "\n",
    "        eval_indicators.append(dict(\n",
    "            category=category,\n",
    "            model=model_name,\n",
    "            exact_match = exact_match(truth, pred),\n",
    "            accuracy = accuracy(truth, pred),\n",
    "            recall = recall(truth, pred),\n",
    "            precision = precision(truth, pred),\n",
    "        ))\n",
    "    metric_df = pl.DataFrame(eval_indicators)\n",
    "\n",
    "    # calculate metrics per category\n",
    "    metric_per_category = metric_df.group_by(\"category\").agg(\n",
    "        pl.col(\"category\").count().alias(\"count\"),\n",
    "        pl.col(\"exact_match\").mean().alias(\"exact_match\"),\n",
    "        pl.col(\"accuracy\").mean().alias(\"accuracy\"),\n",
    "        pl.col(\"recall\").mean().alias(\"recall\"),\n",
    "        pl.col(\"precision\").mean().alias(\"precision\"),\n",
    "    ).with_columns(\n",
    "        (2 / (1 / pl.col(\"recall\") +  1 / pl.col(\"precision\"))).alias(\"f1\"),\n",
    "    ).sort(\"category\")\n",
    "\n",
    "    # calculate metrics\n",
    "    metric_df = (\n",
    "        metric_df\n",
    "        .select(\"exact_match\", \"accuracy\", \"recall\", \"precision\")\n",
    "    )\n",
    "    metric_all = (\n",
    "        metric_df.select(\"exact_match\", \"accuracy\", \"recall\", \"precision\").sum() / len(eval_indicators)\n",
    "    ).with_columns(\n",
    "        (2 / (1 / pl.col(\"recall\") +  1 / pl.col(\"precision\"))).alias(\"f1\"),\n",
    "    )\n",
    "\n",
    "    return metric_all, metric_per_category\n",
    "\n",
    "\n",
    "def evaluate(pred_sheet_id, target_category='color', display_result=True):\n",
    "    # load_data\n",
    "    truth_df = load_truth_data()\n",
    "    pred_gsheet = load_pred_data(pred_sheet_id)\n",
    "    pred_df: pl.DataFrame = pl.from_pandas(\n",
    "        get_as_dataframe(pred_gsheet.worksheet(f\"model_{target_category}\"))\n",
    "    ) # type: ignore\n",
    "\n",
    "    # create validation data\n",
    "    agg_pred_df = (\n",
    "        pred_df\n",
    "        .group_by('model')\n",
    "        .agg(pl.col(target_category).alias(\"truth\"))\n",
    "    )\n",
    "    agg_truth_df = (\n",
    "        truth_df\n",
    "        .group_by('category', 'model')\n",
    "        .agg(pl.col(target_category).alias(\"pred\"))\n",
    "    )\n",
    "    validation_df = (\n",
    "        agg_pred_df\n",
    "        .join(agg_truth_df, on='model', how='left')\n",
    "        .select(\"category\", \"model\", \"truth\", \"pred\")\n",
    "    )\n",
    "    # calculate metrics\n",
    "    metric_all, metric_per_category = calc_metrics(validation_df)\n",
    "    if display_result:\n",
    "        # display(validation_df.head(5))\n",
    "        display(metric_all)\n",
    "        display(metric_per_category)\n",
    "    return metric_all, metric_per_category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>exact_match</th><th>accuracy</th><th>recall</th><th>precision</th><th>f1</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0.289855</td><td>0.465346</td><td>0.565942</td><td>0.48712</td><td>0.523581</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 5)\n",
       "┌─────────────┬──────────┬──────────┬───────────┬──────────┐\n",
       "│ exact_match ┆ accuracy ┆ recall   ┆ precision ┆ f1       │\n",
       "│ ---         ┆ ---      ┆ ---      ┆ ---       ┆ ---      │\n",
       "│ f64         ┆ f64      ┆ f64      ┆ f64       ┆ f64      │\n",
       "╞═════════════╪══════════╪══════════╪═══════════╪══════════╡\n",
       "│ 0.289855    ┆ 0.465346 ┆ 0.565942 ┆ 0.48712   ┆ 0.523581 │\n",
       "└─────────────┴──────────┴──────────┴───────────┴──────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>category</th><th>count</th><th>exact_match</th><th>accuracy</th><th>recall</th><th>precision</th><th>f1</th></tr><tr><td>str</td><td>u32</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>null</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;IoT端末&quot;</td><td>2</td><td>0.5</td><td>0.833333</td><td>0.833333</td><td>1.0</td><td>0.909091</td></tr><tr><td>&quot;ガラケー&quot;</td><td>10</td><td>0.6</td><td>0.77</td><td>0.88</td><td>0.813333</td><td>0.845354</td></tr><tr><td>&quot;スマートフォン&quot;</td><td>43</td><td>0.27907</td><td>0.505633</td><td>0.641473</td><td>0.522742</td><td>0.576053</td></tr><tr><td>&quot;タブレット&quot;</td><td>4</td><td>0.25</td><td>0.25</td><td>0.25</td><td>0.25</td><td>0.25</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 7)\n",
       "┌────────────────┬───────┬─────────────┬──────────┬──────────┬───────────┬──────────┐\n",
       "│ category       ┆ count ┆ exact_match ┆ accuracy ┆ recall   ┆ precision ┆ f1       │\n",
       "│ ---            ┆ ---   ┆ ---         ┆ ---      ┆ ---      ┆ ---       ┆ ---      │\n",
       "│ str            ┆ u32   ┆ f64         ┆ f64      ┆ f64      ┆ f64       ┆ f64      │\n",
       "╞════════════════╪═══════╪═════════════╪══════════╪══════════╪═══════════╪══════════╡\n",
       "│ null           ┆ 0     ┆ 0.0         ┆ 0.0      ┆ 0.0      ┆ 0.0       ┆ 0.0      │\n",
       "│ IoT端末        ┆ 2     ┆ 0.5         ┆ 0.833333 ┆ 0.833333 ┆ 1.0       ┆ 0.909091 │\n",
       "│ ガラケー       ┆ 10    ┆ 0.6         ┆ 0.77     ┆ 0.88     ┆ 0.813333  ┆ 0.845354 │\n",
       "│ スマートフォン ┆ 43    ┆ 0.27907     ┆ 0.505633 ┆ 0.641473 ┆ 0.522742  ┆ 0.576053 │\n",
       "│ タブレット     ┆ 4     ┆ 0.25        ┆ 0.25     ┆ 0.25     ┆ 0.25      ┆ 0.25     │\n",
       "└────────────────┴───────┴─────────────┴──────────┴──────────┴───────────┴──────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric_all, metric_per_category = evaluate(PRED_SS_ID, 'color')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
